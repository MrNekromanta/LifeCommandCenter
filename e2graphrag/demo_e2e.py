"""
E²GraphRAG End-to-End Demo — LifeCommandCenter
Task 2.8: Verify pipeline works on sample data (CPU, Claude API for tree)

Steps:
1. Chunk sample text
2. Extract graph (SpacyExtractor — default E²GraphRAG)
3. Build summary tree (Claude API adapter)
4. Query (Retriever — local + dense)
"""
import os
import sys
import json
import time

# Add project root to path
sys.path.insert(0, os.path.dirname(__file__))

from extract_graph import SpacyExtractor, extract_graph, build_graph
from lcc_extractor import LCCExtractor
from build_tree import build_tree, load_cache_summary
from query import Retriever
from utils import sequential_split

# ── Sample data: excerpt from LCC audit (shortened for demo) ──
SAMPLE_TEXT = """
LifeCommandCenter is a cross-project productivity system that integrates Trello task management 
with Claude analysis through a three-layer architecture. The STATE layer syncs Trello cards via 
FastAPI to a PostgreSQL database every 60 minutes. The KNOWLEDGE layer uses E²GraphRAG to build 
entity graphs and summary trees from conversation audits. The AGENCY layer runs n8n workflows 
for daily digests, stale task detection, and coaching alerts.

Krzysiek, the creator of LifeCommandCenter, works as a PM at ALLMEDICA on EU fintech projects. 
He uses a Beelink SER9 server with 32GB RAM running Windows as the primary development machine. 
The project documentation is stored in Google Drive and synced across devices.

The entity extraction uses a hybrid approach with three layers. Layer 1 is SpaCy with 
pl_core_news_lg for Polish and en_core_web_lg for English NER. Layer 2 is an EntityRuler with 
591 patterns covering 183 domain-specific entities like project names, tools, and concepts. 
Layer 3 is Claude Sonnet API, called selectively for chunks with fewer than 5 entities from 
layers 1 and 2.

BiznesValidator is another project by Krzysiek — a SaaS business validation platform that helps 
entrepreneurs validate their business ideas through automated market analysis, competitor research, 
and financial projections. It uses Supabase for the backend and Cloudflare for CDN and security.

The n8n automation platform is used across multiple projects for workflow automation. In 
LifeCommandCenter, n8n will provide the proactive agency layer with daily digest emails, 
stale task alerts when cards remain in progress for more than 14 days, and weekly coaching 
insights generated by Claude API.

Trello serves as the primary task management GUI. Each project has its own board with 
standardized columns: Backlog, Ready, In Progress with WIP limit of 3, Review, and Done. 
Labels follow a convention with blue, green, and purple for epics, red, yellow, and gray 
for priorities P1 through P3, orange for bugs, pink for ideas, and black for blocked items.
""".strip()

# ── Config ──
CACHE_FOLDER = None  # Set in __main__ based on extractor choice
CHUNK_LENGTH = 300  # tokens (smaller for demo)
CHUNK_OVERLAP = 50


def simple_split(text: str, length: int, overlap: int) -> list:
    """Simple word-based chunking (no HF tokenizer needed for demo)."""
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = start + length
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        if end >= len(words):
            break
        start += length - overlap
    return chunks


def demo_graph_extraction(chunks, use_hybrid=True):
    """Step 1: Extract entity graph."""
    extractor_name = "LCCExtractor (Hybrid)" if use_hybrid else "SpacyExtractor"
    print("\n" + "="*60)
    print(f"STEP 1: Graph Extraction ({extractor_name})")
    print("="*60)
    
    if use_hybrid:
        nlp = LCCExtractor(enable_llm=False)
        print(f"  LCCExtractor loaded (L1: SpaCy, L2: EntityRuler, L3: off)")
    else:
        nlp = SpacyExtractor("en")
        print(f"  SpaCy model loaded: en_core_web_lg")
    
    t0 = time.time()
    (G, index, appearance_count), time_cost = extract_graph(
        chunks, CACHE_FOLDER, nlp, use_cache=False, reextract=True
    )
    elapsed = time.time() - t0
    
    print(f"  Chunks processed: {len(chunks)}")
    print(f"  Graph nodes: {G.number_of_nodes()}")
    print(f"  Graph edges: {G.number_of_edges()}")
    print(f"  Unique entities in index: {len(index)}")
    print(f"  Time: {elapsed:.1f}s")
    
    # Show top entities
    sorted_entities = sorted(appearance_count.items(), key=lambda x: x[1] if isinstance(x[1], int) else 0, reverse=True)
    # Filter out leaf_ keys
    top = [(k,v) for k,v in sorted_entities if not k.startswith("leaf_") and isinstance(v, int)][:15]
    print(f"\n  Top 15 entities:")
    for name, count in top:
        print(f"    {name}: {count}")
    
    return G, index, appearance_count, nlp


def demo_tree_building_claude(chunks):
    """Step 2: Build summary tree using Claude API."""
    print("\n" + "="*60)
    print("STEP 2: Summary Tree (Claude API adapter)")
    print("="*60)
    
    from dotenv import load_dotenv
    import anthropic
    
    # Load API key from hybrid-extractor/.env (shared)
    env_path = os.path.join(os.path.dirname(__file__), "..", "hybrid-extractor", ".env")
    if not os.path.exists(env_path):
        env_path = os.path.join(os.path.dirname(__file__), ".env")
    load_dotenv(env_path)
    
    client = anthropic.Anthropic()
    
    def claude_summarize(text: str, is_leaf: bool = True) -> str:
        """Summarize using Claude API instead of local HF model."""
        if is_leaf:
            system = "You are a summarizer. Summarize the following text, preserving key entities, decisions, and relationships. Be concise but comprehensive. 2-4 sentences."
            prompt = f"Summarize this text:\n\n{text}"
        else:
            system = "You are a summarizer. Combine and summarize these summaries into a higher-level overview. Preserve the most important entities and relationships. 2-3 sentences."
            prompt = f"Combine these summaries:\n\n{text}"
        
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=300,
            system=system,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.content[0].text
    
    # Build tree manually (adapted from build_tree.py)
    merge_num = 3  # merge 3 chunks per summary
    cache = {}
    
    # Leaf nodes
    for i, chunk in enumerate(chunks):
        cache[f"leaf_{i}"] = {
            "text": chunk,
            "children": None,
            "parent": None,
        }
    
    # Level 0: summarize leaf groups
    print(f"  Building level 0 summaries ({len(chunks)} chunks, merge_num={merge_num})...")
    summary_count = 0
    t0 = time.time()
    
    for i in range(0, len(chunks), merge_num):
        group = chunks[i:i+merge_num]
        merged = "\n\n".join(group)
        summary = claude_summarize(merged, is_leaf=True)
        
        children = [f"leaf_{j}" for j in range(i, min(i+merge_num, len(chunks)))]
        cache[f"summary_0_{summary_count}"] = {
            "text": summary,
            "children": children,
            "parent": None,
        }
        for child_id in children:
            cache[child_id]["parent"] = f"summary_0_{summary_count}"
        
        summary_count += 1
        print(f"    summary_0_{summary_count-1}: {summary[:80]}...")
    
    # Level 1+: merge summaries until few enough
    level = 1
    to_summarize = [f"summary_0_{i}" for i in range(summary_count)]
    
    while len(to_summarize) > merge_num:
        print(f"  Building level {level} summaries ({len(to_summarize)} inputs)...")
        new_count = 0
        new_ids = []
        
        for i in range(0, len(to_summarize), merge_num):
            group_ids = to_summarize[i:i+merge_num]
            group_texts = [cache[sid]["text"] for sid in group_ids]
            merged = "\n\n".join(group_texts)
            summary = claude_summarize(merged, is_leaf=False)
            
            sid = f"summary_{level}_{new_count}"
            cache[sid] = {
                "text": summary,
                "children": group_ids,
                "parent": None,
            }
            for child_id in group_ids:
                cache[child_id]["parent"] = sid
            
            new_ids.append(sid)
            new_count += 1
        
        to_summarize = new_ids
        level += 1
    
    # Final root summary if needed
    if len(to_summarize) > 1:
        print(f"  Building root summary...")
        group_texts = [cache[sid]["text"] for sid in to_summarize]
        merged = "\n\n".join(group_texts)
        root_summary = claude_summarize(merged, is_leaf=False)
        
        root_id = f"summary_{level}_0"
        cache[root_id] = {
            "text": root_summary,
            "children": to_summarize,
            "parent": None,
        }
        for child_id in to_summarize:
            cache[child_id]["parent"] = root_id
    
    elapsed = time.time() - t0
    
    # Save cache
    tree_path = os.path.join(CACHE_FOLDER, "tree.json")
    with open(tree_path, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)
    
    n_leaves = sum(1 for k in cache if k.startswith("leaf_"))
    n_summaries = sum(1 for k in cache if k.startswith("summary_"))
    print(f"\n  Tree built: {n_leaves} leaves, {n_summaries} summaries, {level} levels")
    print(f"  Time: {elapsed:.1f}s")
    print(f"  Saved to: {tree_path}")
    
    return cache


def demo_query(cache_tree, G, index, appearance_count, nlp):
    """Step 3: Query the built index."""
    print("\n" + "="*60)
    print("STEP 3: Query (Retriever)")
    print("="*60)
    
    # Initialize retriever (CPU, no embedder for now to avoid large download)
    retriever = Retriever(
        cache_tree, G, index, appearance_count, nlp,
        device="cpu",
        merge_num=3,
        overlap=50,
        shortest_path_k=4,
        debug=True,
        max_chunk_setting=10,
        tokenizer="gpt2",          # lightweight tokenizer for token counting
        embedder="all-MiniLM-L6-v2",  # ~80MB, for dense retrieval fallback
    )
    
    queries = [
        "What is LifeCommandCenter?",
        "What tools does Krzysiek use?",
        "How does entity extraction work?",
    ]
    
    for q in queries:
        print(f"\n  Q: {q}")
        try:
            result = retriever.query(q, shortest_path_k=4, max_chunk_setting=10, debug=True)
            rtype = result.get("retrieval_type", "unknown")
            entities = result.get("entities", [])
            n_chunks = result.get("len_chunks", 0)
            chunks_text = result.get("chunks", "")[:200]
            print(f"  Retrieval type: {rtype}")
            print(f"  Entities found: {entities[:10]}")
            print(f"  Chunks: {n_chunks}")
            print(f"  Preview: {chunks_text}...")
        except Exception as e:
            print(f"  Error: {e}")


if __name__ == "__main__":
    import sys
    sys.stdout.reconfigure(encoding='utf-8')
    
    use_hybrid = "--spacy" not in sys.argv
    
    # Set cache folder per extractor type
    cache_name = "demo_cache_hybrid" if use_hybrid else "demo_cache"
    CACHE_FOLDER = os.path.join(os.path.dirname(__file__), cache_name)
    os.makedirs(CACHE_FOLDER, exist_ok=True)
    
    print("E²GraphRAG End-to-End Demo")
    print(f"Extractor: {'LCCExtractor (Hybrid)' if use_hybrid else 'SpacyExtractor (baseline)'}")
    print(f"Cache: {cache_name}/")
    print(f"Sample text: {len(SAMPLE_TEXT)} chars")
    
    # Chunk
    chunks = simple_split(SAMPLE_TEXT, CHUNK_LENGTH, CHUNK_OVERLAP)
    print(f"Chunks: {len(chunks)} (length={CHUNK_LENGTH}, overlap={CHUNK_OVERLAP})")
    
    # Step 1: Graph
    G, index, appearance_count, nlp = demo_graph_extraction(chunks, use_hybrid=use_hybrid)
    
    # Step 2: Tree (Claude API)
    cache_tree = demo_tree_building_claude(chunks)
    
    # Step 3: Query
    demo_query(cache_tree, G, index, appearance_count, nlp)
    
    print("\n" + "="*60)
    print("DEMO COMPLETE")
    print("="*60)
